{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61c4083b-e0a4-4738-b9ef-96fb02f99b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file pima-indians-diabetes.csv with 768 rows and 9 columns.\n"
     ]
    }
   ],
   "source": [
    "# Loading and scaling datasets\n",
    "# Self-paced exercises followed from the following book :\n",
    "# Machine Learning Algorithms from Scratch with Python (by Jason Brownlee)\n",
    "# This notebook demonstrates sample code from chapters 1 and 2\n",
    "\n",
    "from csv import reader\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    file = open(filename, 'r')\n",
    "    lines = reader(file)\n",
    "    dataset = list(lines)\n",
    "    return dataset\n",
    "\n",
    "# Load dataset\n",
    "filename = 'pima-indians-diabetes.csv'\n",
    "dataset = load_csv(filename)\n",
    "print('Loaded data file {0} with {1} rows and {2} columns.'.format(filename, len(dataset), len(dataset[0])))\n",
    "\n",
    "# Note : This function will load empty lines as valid rows of data. This is undesirable behavior and we can improve the function\n",
    "# to properly take care of empty data rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebe9a919-de37-4398-8e7f-e7e8b426801b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file pima-indians-diabetes.csv with 768 rows and 9 columns.\n",
      "['6', '148', '72', '35', '0', '33.6', '0.627', '50', '1']\n",
      "[6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "from csv import reader\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "# Load Pima Indians Diabetes dataset\n",
    "filename = 'pima-indians-diabetes.csv'\n",
    "dataset = load_csv(filename)\n",
    "print('Loaded data file {0} with {1} rows and {2} columns.'.format(filename, len(dataset), len(dataset[0])))\n",
    "\n",
    "# Note: The reader function in csv module loads data columns as string values. To enable further processing of data,\n",
    "# we need to convert values to float. The following output shows raw data format we get from using csv module.\n",
    "print(dataset[0])\n",
    "\n",
    "# We can use our simple function to improve runtime data format. This can be done using our simple function.\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53939c83-72ed-4d31-85e7-46e5d061aad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file iris.csv with 150 rows and 5 columns.\n",
      "[5.1, 3.5, 1.4, 0.2, 'Iris-setosa']\n",
      "[5.1, 3.5, 1.4, 0.2, 2]\n",
      "{'Iris-versicolor': 0, 'Iris-virginica': 1, 'Iris-setosa': 2}\n"
     ]
    }
   ],
   "source": [
    "from csv import reader\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n",
    "# Note: We're now using Iris dataset, which has class labels in the last column. Because numeric values are preferred,\n",
    "# we defined a simple function that maps each unique class label to an integer. We now load and process Iris dataset.\n",
    "\n",
    "# Load Iris dataset\n",
    "filename = 'iris.csv'\n",
    "dataset = load_csv(filename)\n",
    "print('Loaded data file {0} with {1} rows and {2} columns.'.format(filename, len(dataset), len(dataset[0])))\n",
    "\n",
    "# Convert string columns to float (here, we must process feature columns only; i.e. columns 1 to 4)\n",
    "for i in range(4):\n",
    "    str_column_to_float(dataset, i)\n",
    "\n",
    "# Our dataset looks good, but class labels are still in string form, as shown in the following output\n",
    "print(dataset[0])\n",
    "\n",
    "# Convert class labels to nominal integer values\n",
    "lookup = str_column_to_int(dataset, 4)\n",
    "\n",
    "# The Iris dataset is now better prepared for further processing in ML algorithms\n",
    "print(dataset[0])\n",
    "print(lookup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6137139d-c796-4614-952e-53ec76cf2ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: In practice, we'll always use proven tools and libraries for common ML tasks like loading and cleaning data.\n",
    "# The above sample codes are for demonstration purpose only. Common ML stack libraries, like Numpy and Pandas are much\n",
    "# more robust and flexible to use in Data Preparation stage of the machine learning workflow.\n",
    "\n",
    "# Research Notes :\n",
    "# There are many areas that require additional research and experiment, such as these important considerations :\n",
    "# 1. Other sources of data (like SQL tables and views, NoSQL collections, Graph databases, Streaming data, etc.)\n",
    "# 2. Mapping input data to a format suitable for later stages\n",
    "# 3. Common data cleansing concerns (missing data, invalid data, malformed data, etc.)\n",
    "\n",
    "# Chapter 1 of JB book is now complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ddc0909-3a73-4eed-8a03-f9a3ef6cb1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 2  : Scale Machine Learning Data\n",
    "# Book title : Machine Learning Algorithms from Scratch with Python (by Jason Brownlee)\n",
    "\n",
    "# Note: Many ML algorithms need input (and output) data to fall inside a limited range (usually : 0 to 1 or -1 to 1).\n",
    "# This is called scaling the data.\n",
    "# There are two basic techniques for scaling data, namely : Normalization and Standardization\n",
    "\n",
    "# **** Normalization ****\n",
    "# This technique requires minimum and maximum values for each feature in the dataset.\n",
    "# It's pretty easy to prepare for normalization. We need a function to calculate min-max values for all features.\n",
    "# We also need a function to rescale a dataset to normal form (i.e. normalizing each column in each row)\n",
    "\n",
    "# Find the min and max values for each column in the given dataset\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    for i in range(len(dataset[0])):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        min_value = min(col_values)\n",
    "        max_value = max(col_values)\n",
    "        minmax.append([min_value, max_value])\n",
    "    return minmax\n",
    "\n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c49ff8c-7bf3-49a1-a388-e2d8b3956d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file pima-indians-diabetes.csv with 768 rows and 9 columns.\n",
      "[6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0, 1.0]\n",
      "[0.35294117647058826, 0.7437185929648241, 0.5901639344262295, 0.35353535353535354, 0.0, 0.5007451564828614, 0.23441502988898377, 0.48333333333333334, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Note: We're going to use functions from chapter 1 to load Diabetes dataset, convert values to float and rescale dataset\n",
    "# It's good to think about the whole process as a pipeline with different stages. Here, we have the following stages in our pipeline :\n",
    "# Stage 1 - Data Acquisition : Import data from external source (i.e. CSV data file)\n",
    "# Stage 2 - Data Preparation : Convert string values to floating point numbers\n",
    "# Stage 3 - Data Preparation : Rescale dataset columns using Normalization technique\n",
    "\n",
    "from csv import reader\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "# Find the min and max values for each column in the given dataset\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    for i in range(len(dataset[0])):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        min_value = min(col_values)\n",
    "        max_value = max(col_values)\n",
    "        minmax.append([min_value, max_value])\n",
    "    return minmax\n",
    "\n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "\n",
    "# Sample pipeline demo\n",
    "# Stage 1 - Data Acquisition : Import data from external source (i.e. CSV data file)\n",
    "filename = 'pima-indians-diabetes.csv'\n",
    "dataset = load_csv(filename)\n",
    "print('Loaded data file {0} with {1} rows and {2} columns.'.format(filename, len(dataset), len(dataset[0])))\n",
    "\n",
    "# Stage 2 - Data Preparation : Convert string values to floating point numbers\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "print(dataset[0])\n",
    "\n",
    "# Stage 3 - Data Preparation : Rescale dataset columns using Normalization technique\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7027c634-6f46-41f0-8429-81269f5c13c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **** Standardization ****\n",
    "# This technique requires special statistical values; namely, Mean and Standard Deviation, for each feature in the dataset.\n",
    "# We need to implement 3 functions to support standardization technique. These functions are summarized below :\n",
    "# 1. column_means   : Calculates mean value for each column in dataset\n",
    "# 2. column_stddevs : Calculates standard deviation for each column in dataset\n",
    "# 3. standardize_dataset : Rescales all feature values in a dataset using standardization formula\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "# Calculate column means\n",
    "def column_means(dataset):\n",
    "    means = [0 for i in range(len(dataset[0]))]\n",
    "    for i in range(len(dataset[0])):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        means[i] = sum(col_values) / float(len(dataset))\n",
    "    return means\n",
    "\n",
    "# Calculate column standard deviations\n",
    "def column_stddevs(dataset, means):\n",
    "    stddevs = [0 for i in range(len(dataset[0]))]\n",
    "    for i in range(len(dataset[0])):\n",
    "        variance = [pow(row[i] - means[i], 2) for row in dataset]\n",
    "        stddevs[i] = sum(variance)\n",
    "    stddevs = [sqrt(x / float(len(dataset) - 1)) for x in stddevs]\n",
    "    return stddevs\n",
    "\n",
    "# Standardize dataset\n",
    "def standardize_dataset(dataset, means, stddevs):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i] - means[i]) / stddevs[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "619f9c35-fe23-49e5-a318-36b7bd49ae14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file pima-indians-diabetes.csv with 768 rows and 9 columns.\n",
      "[6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0, 1.0]\n",
      "[0.6395304921176576, 0.8477713205896718, 0.14954329852954296, 0.9066790623472505, -0.692439324724129, 0.2038799072674717, 0.468186870229798, 1.4250667195933604, 1.3650063669598067]\n"
     ]
    }
   ],
   "source": [
    "# Note: We're going to use functions from chapter 1 to load Diabetes dataset, convert values to float and rescale dataset\n",
    "# We have the same stages as we had in normalization demo :\n",
    "# Stage 1 - Data Acquisition : Import data from external source (i.e. CSV data file)\n",
    "# Stage 2 - Data Preparation : Convert string values to floating point numbers\n",
    "# Stage 3 - Data Preparation : Rescale dataset columns using Standardization technique\n",
    "\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "# Calculate column means\n",
    "def column_means(dataset):\n",
    "    means = [0 for i in range(len(dataset[0]))]\n",
    "    for i in range(len(dataset[0])):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        means[i] = sum(col_values) / float(len(dataset))\n",
    "    return means\n",
    "\n",
    "# Calculate column standard deviations\n",
    "def column_stddevs(dataset, means):\n",
    "    stddevs = [0 for i in range(len(dataset[0]))]\n",
    "    for i in range(len(dataset[0])):\n",
    "        variance = [pow(row[i] - means[i], 2) for row in dataset]\n",
    "        stddevs[i] = sum(variance)\n",
    "    stddevs = [sqrt(x / float(len(dataset) - 1)) for x in stddevs]\n",
    "    return stddevs\n",
    "\n",
    "# Standardize dataset\n",
    "def standardize_dataset(dataset, means, stddevs):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i] - means[i]) / stddevs[i]\n",
    "\n",
    "# Sample pipeline demo\n",
    "# Stage 1 - Data Acquisition : Import data from external source (i.e. CSV data file)\n",
    "filename = 'pima-indians-diabetes.csv'\n",
    "dataset = load_csv(filename)\n",
    "print('Loaded data file {0} with {1} rows and {2} columns.'.format(filename, len(dataset), len(dataset[0])))\n",
    "\n",
    "# Stage 2 - Data Preparation : Convert string values to floating point numbers\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "print(dataset[0])\n",
    "\n",
    "# Stage 3 - Data Preparation : Rescale dataset columns using Standardization technique\n",
    "means = column_means(dataset)\n",
    "stddevs = column_stddevs(dataset, means)\n",
    "standardize_dataset(dataset, means, stddevs)\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe499360-9a69-4118-aee7-7a6778433dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Normal and Standard scaling are the two basic techniques for handling large variations in numeric data.\n",
    "# There are many more transforms that we may need to apply, depending on useful insight we can get from basic data analysis.\n",
    "# Statistical analysis and summaries, as well as many available visualizations, can give us this valuable insight.\n",
    "# Preparatory stages in a typical machine learning workflow require a lot of knowledge and experience that can only be gained\n",
    "# by research and practice.\n",
    "\n",
    "# Chapter 2 of JB book is now complete."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
